


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


df = pd.read_csv('insurance.csv')
df.info()





df.columns


df.isna().sum()


cat = df.select_dtypes(include=['object']).columns
cat


num = df.select_dtypes(include=['float64', 'int64']).columns
num


feature = ['sex', 'smoker', 'region']
plt.subplots(figsize=(20,10))
for i, col in enumerate(feature):
    plt.subplot(1, 3, i + 1)

    x = df[col].value_counts()
    plt.pie(x.values, labels= x.index, autopct='%1.1f%%')

plt.show()






x = sns.catplot(x='sex', kind='count',data=df)
ax = x.facet_axis(0, 0)
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.show()









x = sns.catplot(x='smoker', kind='count',data=df)
ax = x.facet_axis(0, 0)
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.show()


smoker_distribution = df['smoker'].value_counts(normalize=True)
print("Proportion of smokers versus non-smokers:")
print(smoker_distribution)







x = sns.catplot(x='region', kind='count',data=df)
ax = x.facet_axis(0, 0)
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.show()








# Scatter plot with regression line
sns.jointplot(x='age', y='expenses', data=df, kind='reg')
plt.xlabel('Age')
plt.ylabel('Expenses')
plt.title('Correlation between Age and Expenses')
plt.show()





# Box plot
ax = sns.barplot(x='smoker', y='expenses', data=df, errorbar=None)
plt.xlabel('Smoker')
plt.ylabel('Expenses')
plt.title('Effect of Smoking Status on Expenses')

# Annotate each bar with its height
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),
                textcoords='offset points')
plt.show()








ax = sns.barplot(x='children', y='expenses', data=df, errorbar=None)  # ci=None removes error bars
plt.xlabel('Number of Children')
plt.ylabel('Expenses')
plt.title('Relationship between Number of Children and Expenses')

# Annotate each bar with its height
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.show()



sns.barplot(x='region', y='expenses', data=df,errorbar=None)
plt.xlabel('Region')
plt.ylabel('Expenses')
plt.title('Differences in Expenses across Regions')
plt.show()





import statsmodels.api as sm

# Multiple regression model
X = sm.add_constant(df[['age', 'bmi', 'children']])
y = df['expenses']
model = sm.OLS(y, X).fit()

# Print summary
print(model.summary())






test = ['age', 'bmi', 'children']

for i in test:
    sns.boxplot(df[test])
    plt.title('Checking for outliers in the numerical features')
    

plt.show()





col = 'bmi'
q1 = df[col].quantile(0.25)
q2 = df[col].quantile(0.5)
q3 = df[col].quantile(0.75)

iqr = q3 - q1
low = q1 - 1.5 * iqr
up = q3+1.5*iqr



low


up


# Create a boolean mask for outliers
outlier_mask = (df[col] < low) | (df[col] > up)
outlier_mask


df_no_outliers = df[~outlier_mask]

df_no_outliers


test = ['age', 'bmi']

for i in test:
    sns.boxplot(df_no_outliers[test])

plt.show()


test = ['age', 'bmi', 'expenses']
for i in test:
    x = df[test].skew()

x


categorical = df_no_outliers.select_dtypes(include=['object']).columns
categorical


cat_cols = ['sex', 'smoker', 'region']


from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()


# Iterate through each categorical column and encode its values
for col in cat_cols:
    # Fit LabelEncoder on the unique values in the column and transform the column
    df_no_outliers[col] = label_encoder.fit_transform(df_no_outliers[col])


df_no_outliers


df_no_outliers.shape








# creating the train, validation and test datasets

X = df_no_outliers.drop(['expenses'], axis=1)
y = df_no_outliers[['expenses']]


from sklearn.linear_model import LinearRegression, Lasso
from sklearn.svm import SVR 
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV


l1 = []
l2 = []
l3 = []

cvs = 0
for i in range(0,100):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)
    l1.append(lr_model.score(X_train, y_train))
    l2.append(lr_model.score(X_test, y_test))
    cvs = (cross_val_score(lr_model, X, y, cv=5,)).mean()
    l3.append(cvs)
    df1 = pd.DataFrame({'train_data score':l1, 'test_data_score': l2, 'cvs score':l3})
    


df1 = pd.DataFrame({'train_data score':l1, 'test_data_score': l2, 'cvs score':l3})
df1


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('LinearRegression')
lr_model_train_score = lr_model.score(X_train, y_train)
lr_model_test_score = lr_model.score(X_test, y_test)
lr_model_val_score = cross_val_score(lr_model,X,y,cv=5,).mean()
print('Train data score',lr_model.score(X_train, y_train))
print('Test data score', lr_model.score(X_test, y_test))
print('Validation data score',cross_val_score(lr_model,X,y,cv=5,).mean())





rfmodel= RandomForestRegressor(random_state=42)

rfmodel.fit(X_train,y_train)


y_pred_train_rf=rfmodel.predict(X_train)

y_pred_test_rf=rfmodel.predict(X_test)

print(r2_score(y_train,y_pred_train_rf))
print(r2_score(y_test,y_pred_test_rf))

print(cross_val_score(rfmodel,X,y,cv=5,).mean())


estimator= RandomForestRegressor(random_state=42)

param_grid= {'n_estimators':[10,30,60,78,90,130,160]}

grid= GridSearchCV(estimator,param_grid,scoring="r2",cv=5)
grid.fit(X_train,y_train)
print(grid.best_params_)


rfmodel=RandomForestRegressor(random_state=42,n_estimators=160)

rfmodel.fit(X_train,y_train)

y_pred_train_rf2=rfmodel.predict(X_train)
y_pred_test_rf2=rfmodel.predict(X_test)
print('RandomForestRegressor')

rf_model_train_score = r2_score(y_train,y_pred_train_rf2)
rf_model_test_score = r2_score(y_test,y_pred_test_rf2)
rf_model_val_score = cross_val_score(rfmodel,X,y,cv=5,).mean()

print(r2_score(y_train,y_pred_train_rf2))

print(r2_score(y_test,y_pred_test_rf2))

print(cross_val_score(rfmodel,X,y,cv=5,).mean())





gb_model = GradientBoostingRegressor()
gb_model.fit(X_train,y_train)


y_pred_train_gb = gb_model.predict(X_train)
y_pred_test_gb = gb_model.predict(X_test)


print(r2_score(y_train,y_pred_train_gb))
print(r2_score(y_test,y_pred_test_gb))
print(cross_val_score(gb_model,X,y,cv=5,).mean())


estimator = GradientBoostingRegressor()

param_grid={'n_estimators':[5,15,19,30,50,90],'learning_rate':[0.1,0.13,0.2,0.25,0.9,1]}

grid=GridSearchCV(estimator,param_grid,scoring="r2",cv=5)

grid.fit(X_train,y_train)

print(grid.best_params_)


gbmodel=GradientBoostingRegressor(n_estimators=55,learning_rate=0.1)

gbmodel.fit(X_train,y_train)
y_pred_train_gb= gbmodel.predict(X_train)
y_pred_test_gb = gbmodel.predict(X_test)


print(r2_score(y_train,y_pred_train_gb))

print(r2_score(y_test,y_pred_test_gb))

print(cross_val_score(gbmodel,X,y,cv=5,).mean())


gb_model_train_score = r2_score(y_train,y_pred_train_gb)
gb_model_test_score = r2_score(y_test,y_pred_test_gb)
gb_model_val_score = cross_val_score(gbmodel,X,y,cv=5,).mean()


xgmodel= XGBRegressor()
xgmodel.fit(X_train,y_train)



y_pred_train_xgb = xgmodel.predict(X_train)
y_pred_test_xgb = xgmodel.predict(X_test)

print(r2_score(y_train,y_pred_train_xgb))

print(r2_score(y_test,y_pred_test_xgb))

print(cross_val_score(xgmodel,X,y,cv=5,).mean())


estimator=XGBRegressor()

param_grid={'n_estimators':[10,30,50,70,90],'max_depth':[3,4,5],'gamma':[0,0.15,0.3,0.5,1]}

grid = GridSearchCV(estimator,param_grid,scoring="r2",cv=5)

grid.fit(X_train,y_train)
print(grid.best_params_)


xgb_model = XGBRegressor(n_estimators=10,max_depth=3,gamma=0)
xgb_model.fit(X_train,y_train)
y_pred_train_xgb=xgb_model.predict(X_train)
y_pred_test_xgb=xgb_model.predict(X_test)



print(r2_score(y_train,y_pred_train_xgb))
print(r2_score(y_test,y_pred_test_xgb))
print(cross_val_score(xgb_model,X,y,cv=5,).mean())


xgb_model_train_score = r2_score(y_train,y_pred_train_xgb)
xgb_model_test_score = r2_score(y_test,y_pred_test_xgb)
xgb_model_val_score = cross_val_score(xgb_model,X,y,cv=5,).mean()





train_score = []
test_score = []
validation_score = []
model_name = []


train_score


#linear regression
train_score.append(lr_model_train_score)
test_score.append(lr_model_test_score)
validation_score.append(lr_model_val_score)
model_name.append('LinearRegression')


model_name


#randomforestregressor
train_score.append(rf_model_train_score)
test_score.append(rf_model_test_score)
validation_score.append(rf_model_val_score)
model_name.append('RandomForestRegressor')




#gradient boosting
train_score.append(gb_model_train_score)
test_score.append(gb_model_test_score)
validation_score.append(gb_model_val_score)
model_name.append("GradientBoostingRegressor")


# xgb regressor model
train_score.append(xgb_model_train_score)
test_score.append(xgb_model_test_score)
validation_score.append(xgb_model_val_score)
model_name.append("XGBoostRegressor")


# model table
model_table = pd.DataFrame({
    'model_name': model_name,
    'model_train_score' : train_score,
    'model_test_score' : test_score,
    'model validation score' : validation_score})
model_table





feature_importance = xgb_model.feature_importances_

# Map Feature Importance to Features
feature_names = list(X.columns)
feature_importance_map = dict(zip(feature_names, feature_importance))

# Sort and Visualize
sorted_features = sorted(feature_importance_map.items(), key=lambda x: x[1], reverse=True)
top_features = [feature[0] for feature in sorted_features]
top_importance = [feature[1] for feature in sorted_features]

plt.barh(top_features, top_importance)
plt.xlabel('Feature Importance')
plt.title('Top Features in XGBoost Regressor Model')
plt.savefig('rf_features.png')
plt.show()



xgb_model_features= pd.DataFrame({'Feature': top_features, 'Importance': top_importance})
xgb_model_features = xgb_model_features.sort_values(by='Importance', ascending=False)
xgb_model_features


important_features = xgb_model_features[xgb_model_features['Importance']>0.01]
important_features



df_no_outliers.columns


df_no_outliers.drop(df_no_outliers[['sex','region']],axis=1,inplace=True)


xf = df_no_outliers.drop(df_no_outliers[['expenses']], axis=1)
x = df_no_outliers.drop(df_no_outliers[['expenses']], axis=1)


xtrain, xtest, ytrain, ytest = train_test_split(xf, y, test_size=0.3, random_state=42)
finalmodel = XGBRegressor(n_estimators=10, max_depth =3, gamma=0)
finalmodel.fit(xtrain, ytrain)


y_pred_train=finalmodel.predict(xtrain)
y_pred_test=finalmodel.predict(xtest)
print(r2_score(ytrain,y_pred_train))
print(r2_score(ytest,y_pred_test))
print(cross_val_score(finalmodel,X,y,cv=5,).mean())


from pickle import dump
dump(finalmodel,open('insurancemodelf.pkl','wb'))



new_data=pd.DataFrame({'age':49,'sex':'female','bmi':27.9,'children':4,'smoker':'yes','region':'northeast'},index=[0])
new_data['smoker']=new_data['smoker'].map({'yes':1,'no':0})
new_data=new_data.drop(new_data[['sex','region']],axis=1)
finalmodel.predict(new_data)




